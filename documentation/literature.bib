@article{Barbedo2007,
    abstract = {We present a strategy to perform automatic genre classification of musical signals. The technique divides the signals into 21.3 milliseconds frames, from which 4 features are extracted. The values of each feature are treated over 1-second analysis segments. Some statistical results of the features along each analysis segment are used to determine a vector of summary features that charac- terizes the respective segment.Next, a classification procedure uses those vectors to differentiate between genres. The classification procedure has twomain characteristics: (1) a very wide and deep taxonomy, which allows a very meticulous comparison between different genres, and (2) a wide pairwise comparison of genres, which allows emphasizing the differences between each pair of genres. The procedure points out the genre that best fits the characteristics of each segment. The final classification of the signal is given by the genre that appearsmore times along all signal segments. The approach has shown very good accuracy even for the lowest layers of the hierarchical structure.},
    author = {Barbedo, Jayme Garcia Arnal and Lopes, Amauri},
    doi = {10.1155/2007/64960},
    file = {::},
    issn = {11108657},
    journal = {Eurasip Journal on Advances in Signal Processing},
    title = {{Automatic genre classification of musical signals}},
    volume = {2007},
    year = {2007}
}
@article{Cook2007,
    abstract = {Statistics 503: Data Mining},
    author = {Cook, Dianne},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/8-classification.pdf:pdf},
    journal = {Homework Assignment},
    pages = {1},
    title = {{Music Classification}},
    url = {papers3://publication/uuid/DE560E14-2EA2-4A4A-8887-A238F2232E0E},
    year = {2007}
}
@article{Expose2014,
    author = {Expos\'{e}, Thesis},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/Julian\_Terzyk\_Exp.pdf:pdf},
    number = {December},
    title = {{Audio Feature Selection and Aggregation for Musical Genre Classification}},
    year = {2014}
}
@misc{Jeremic,
    author = {Jeremi\'{c}, Marina},
    title = {{Music classification by genre using neural networks}},
    url = {http://neuroph.sourceforge.net/tutorials/MusicClassification/music\_classification\_by\_genre\_using\_neural\_networks.html},
    urldate = {2015-05-01}
}
@article{Kim2010,
    abstract = {This paper surveys the state of the art in automatic emo- tion recognition in music. Music is oftentimes referred to as a language of emotion 1, and it is natural for us to categorize music in terms of its emotional associations. Myriad features, such as harmony, timbre, interpretation, and lyrics affect emotion, and the mood of a piece may also change over its duration. But in developing automated systems to organize music in terms of emotional content, we are faced with a problem that oftentimes lacks a well- de ned answer; there may be considerable disagreement regarding the perception and interpretation of the emotions of a song or ambiguity within the piece itself. When com- pared to other music information retrieval tasks (e.g., genre identi cation), the identi cation of musical mood is still in its early stages, though it has received increasing attention in recent years. In this paper we explore a wide range of research in music emotion recognition, particularly focus- ing on methods that use contextual text information (e.g., websites, tags, and lyrics) and content-based approaches, as well as systems combining multiple feature domains.},
    author = {Kim, Youngmoo E and Schmidt, Erik M and Migneco, Raymond and Morton, Brandon G and Richardson, Patrick and Scott, Jeffrey and Speck, Jacquelin a and Turnbull, Douglas},
    journal = {Information Retrieval},
    number = {Ismir},
    pages = {255--266},
    title = {{Music Emotion Recognition : a State of the Art Review}},
    url = {http://ismir2010.ismir.net/proceedings/ismir2010-45.pdf},
    year = {2010}
}
@article{Li2003,
    abstract = { Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.},
    author = {Li, Tao Li Tao and Tzanetakis, G.},
    doi = {10.1109/ASPAA.2003.1285840},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/factors-music.pdf:pdf},
    isbn = {0-7803-7850-4},
    journal = {2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No.03TH8684)},
    pages = {29--32},
    title = {{Factors in automatic musical genre classification of audio signals}},
    year = {2003}
}
@article{Mandel2005,
    abstract = {Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we showthat this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. Keywords:},
    author = {Mandel, Michael I. and Ellis, Daniel P. W.},
    doi = {10.1038/sj.embor.7400483},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/ismir05-svm.pdf:pdf},
    isbn = {0955117909},
    issn = {1469221X},
    journal = {Proceedings of the 6th International Symposium in Music Information Retrieval (ISMIR'05)},
    keywords = {artist identification,kernel spaces,song classifica-,support vector machines,tion},
    pages = {594--599},
    title = {{Song-level features and support vector machines for music classification}},
    year = {2005}
}
@article{Mayer2010,
    abstract = {Digital audio has become an almost ubiquitously spread medium, and for many consumers, digital audio is the major distribution and storage form of music. Numerous on-line music stores account for a growing share of record sales. The widespread adoption of digital audio on home computers and especially mobile devices, and numerous on-line music stores show the size of this market. Handling the ever growing size of both private and commercial collections however becomes increasingly difficult. Computer algorithms that can understand and interpret characteristics of music, and organise and recommend them for and to their users can be of great assistance. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. Album covers are carefully designed by artists to convey a message consistent with the music and image of a band. Music videos, fan sites and other sources of information add to that in a usually coherent manner. In this paper, we focus on exploring the lyrics domain of music, and how this information can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, were we successfully demonstrated simple approaches of combining different representations of music, we apply a more sophisticated machine learning technique, ensemble classification. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly.},
    author = {Mayer, Rudolf and Rauber, Andreas},
    isbn = {978-602-97479-0-4},
    journal = {2010 International Conference on Distributed Frameworks for Multimedia Applications},
    pages = {1--6},
    title = {{Building ensembles of audio and lyrics features to improve musical genre classification}},
    url = {http://publik.tuwien.ac.at/files/PubDat\_205736.pdf},
    year = {2010}
}
@article{McEnnis2005,
abstract = {jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straight forward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio ﬁles as input. In the GUI, users select the features that they wish to have extracted—letting jAudio take care of all dependency problems—and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML ﬁle or an ARFF ﬁle depending on the user’s preference.},
author = {McEnnis, Daniel and McKay, Cory and Fujinaga, Ichiro and Depalle, Philippe},
doi = {10.1.1.149.8677},
file = {:C$\backslash$:/Users/Philipp/Desktop/jAudio\_ISMIR\_2005.pdf:pdf},
isbn = {0-9551179-0-9},
journal = {Proceedings of the International Conference on Music Information Retrieval},
keywords = {audio feature,extraction,java audio environment,music information retrieval},
pages = {600--603},
title = {{jAudio: A feature extraction library}},
url = {http://www.music.mcgill.ca/~cmckay/papers/musictech/jAudio\_ISMIR\_2005.pdf},
year = {2005}
}
@article{Mckinney2003,
    abstract = {Four audio feature sets are evaluated in their ability to classify five general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
    author = {Mckinney, Mf Martin F and Breebaart, Jeroen},
    doi = {10.1016/j.asoc.2010.08.017},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/McKinney.PDF:PDF},
    isbn = {2974619401},
    issn = {15684946},
    journal = {Proc ISMIR},
    pages = {151--158},
    title = {{Features for Audio and Music Classification}},
    url = {https://jscholarship.library.jhu.edu/handle/1774.2/22$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.5514\&amp;rep=rep1\&amp;type=pdf},
    volume = {4},
    year = {2003}
}
@article{Nascimento2006,
    abstract = {This paper presents a novel approach to the task of automatic music genre classification which is based on ensemble learning. Feature vectors are extracted from three 30-second music segments from the beginning, middle and end of each music piece. Individual classifiers are trained to account for each music segment. During classification, the output provided by each classifier is combined with the aim of improving music genre classification accuracy. Experiments carried out on a dataset containing 600 music samples from two Latin genres (Tango and Salsa) have shown that for the task of automatic music genre classification, the features extracted from the middle and end music segments provide better results than using the beginning music segment. Furthermore, the proposed ensemble method provides better accuracy than using single classifiers and any individual segment.},
    author = {Nascimento, Carlos and Kaestner, Celso Antonio Alves and Koerich, Alessandro Lameiras},
    doi = {10.1109/ICSMC.2007.4414136},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/AutoSIla.pdf:pdf},
    isbn = {978-1-4244-0990-7},
    journal = {IEEE International Conference on Systems Man and Cybernetics},
    keywords = {computer programming,qa 76 software},
    pages = {1687--1692},
    title = {{Automatic Genre Classification of Latin Music Using Ensemble of Classifiers}},
    url = {http://kar.kent.ac.uk/24094/},
    year = {2006}
}
@article{Nasridinov2014,
    author = {Nasridinov, Aziz and Park, Young-ho},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/4.pdf:pdf},
    journal = {International Journal of Multimedia and Ubiquitous Engineering},
    keywords = {04,10,14257,2014,31-42,9,a study on music,and ubiquitous engineering,doi,dx,ernational journal of multimedia,genre recognition and classification,http,ijmue,no,org,pp,techniques,vol},
    number = {4},
    pages = {31--42},
    title = {{A Study on Music Genre Recognition and Classification Techniques}},
    volume = {9},
    year = {2014}
}
@article{Oord2013,
    author = {Oord, Aaron Van Den and Dieleman, Sander and Schrauwen, Benjamin},
    file = {::},
    journal = {Advances in Neural Information Processing Systems},
    pages = {2643--2651},
    title = {{Deep content-based music recommendation}},
    url = {http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation},
    year = {2013}
}
@article{Paradzinets2009,
    author = {Paradzinets, Aliaksandr and Harb, Hadi and Chen, Liming},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/Liris-4224.pdf:pdf},
    keywords = {Music genre classification,acoustic features,multi expert system,rhythmic features,timbre features},
    number = {June},
    title = {{Multiexpert System for Automatic Music Genre}},
    year = {2009}
}
@inproceedings{Six2014,
  author      = {Joren Six and Olmo Cornelis and Marc Leman},
  title       = {{TarsosDSP, a Real-Time Audio Processing Framework in Java}},
  booktitle   = {{Proceedings of the 53rd AES Conference (AES 53rd)}}, 
  year        =  2014
}
@article{Talupur2003,
    abstract = {Our system successfully determined the genre of the vast majority of the test songs. Not only did the system choose a genre, it quantified its output with a level of sureness.},
    author = {Talupur, Muralidhar and Nath, Suman and Yan, Hong},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/GCfA.pdf:pdf},
    journal = {Building},
    pages = {1--5},
    title = {{Classification of Music Genre}},
    year = {2003}
}
@article{Tellegen1999,
    author = {A. Tellegen and D. Watson and L. A. Clark},
    title = {{On the Dimensional and Hierarchical Structure of Affect}},
    journal = {Psychological Science},
    volume = {10},
    year = {1999},
    pages = {297--303},
    issue = {4},
    doi = {10.1111/1467-9280.00157},
    masid = {2377212}
}
@article{Trohidis2011,
abstract = {This work studies the task of automatic emotion detection in music. Music may evoke more than one different emotion at the same time. Single-label classification and regression cannot model this multiplicity. Therefore, this work focuses on multi-label classification approaches, where a piece of music may simultaneously belong to more than one class. Seven algorithms are experimentally compared for this task. Furthermore, the predictive power of several audio features is evaluated using a new multi-label feature selection method. Experiments are conducted on a set of 593 songs with six clusters of emotions based on the Tellegen-Watson-Clark model of affect. Results show that multi-label modeling is successful and provide interesting insights into the predictive quality of the algorithms and features.},
author = {Trohidis, Konstantinos and Tsoumakas, Grigorios and Kalliris, George and Vlahavas, Ioannis},
doi = {10.1186/1687-4722-2011-426793},
file = {:C$\backslash$:/Users/Philipp/Desktop/1687-4722-2011-426793.pdf:pdf},
issn = {1687-4722},
journal = {EURASIP Journal on Audio, Speech, and Music Processing},
keywords = {feature selection,multi-label classification,music information retrieval},
number = {1},
pages = {4},
publisher = {Springer Open Ltd},
title = {{Multi-label classification of music by emotion}},
url = {http://asmp.eurasipjournals.com/content/pdf/1687-4722-2011-426793.pdf},
volume = {2011},
year = {2011}
}
@article{Tzanetakis2001,
    abstract = {Musical genres are categorical descriptions that are used to describe music. They are commonly used to structure the increasing amounts of music available in digital form on the Web and are important for music information retrieval. Genre categorization for audio has traditionally been performed manually. A particular musical genre is characterized by statistical properties related to the instrumentation, rhythmic structure and form of its members. In this work, algorithms for the automatic genre categorization of audio signals are described. More specifically, we propose a set of features for representing texture and instrumentation. In addition a novel set of features for representing rhythmic structure and strength is proposed. The performance of those feature sets has been evaluated by training statistical pattern recognition classifiers using real world audio collections. Based on the automatic hierarchical genre classification two graphical user interfaces for browsing and interacting with large audio collections have been developed.},
    author = {Tzanetakis, G and Essl, G and Cook, P},
    doi = {10.1109/TSA.2002.800560},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/tzanetakis.pdf:pdf},
    issn = {1063-6676},
    journal = {Proceedings of the Second International Symposium on Music Information Retrieval},
    keywords = {Folder - Feature extraction,file-import-09-02-12,music},
    pages = {6 total pages},
    title = {{Automatic musical genre classification of audio signals}},
    url = {http://ismir2001.ismir.net/pdf/tzanetakis.pdf},
    year = {2001}
}
@article{Tzanetakis2002,
    abstract = { Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61\% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
    author = {Tzanetakis, George and Cook, Perry},
    doi = {10.1109/TSA.2002.800560},
    file = {:C$\backslash$:/Users/Philipp/Documents/Research/Music Reccommendations/tsap02gtzan.pdf:pdf},
    isbn = {1063-6676},
    issn = {10636676},
    journal = {IEEE Transactions on Speech and Audio Processing},
    keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
    number = {5},
    pages = {293--302},
    pmid = {7359731},
    title = {{Musical genre classification of audio signals}},
    volume = {10},
    year = {2002}
}
@article{Tzanetakis1999,
 author = {Tzanetakis, George and Cook, Perry},
 title = {MARSYAS: A Framework for Audio Analysis},
 journal = {Org. Sound},
 issue_date = {December 1999},
 volume = {4},
 number = {3},
 month = dec,
 year = {1999},
 issn = {1355-7718},
 pages = {169--175},
 numpages = {7},
 url = {http://dx.doi.org/10.1017/S1355771800003071},
 doi = {10.1017/S1355771800003071},
 acmid = {972857},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 
@article{Yoshii2006,
    abstract = {This paper presents a hybrid music recommendation method that solves problems of two prominent conventional meth-ods: collaborative filtering and content-based recommen-dation. The former cannot recommend musical pieces that have no ratings because recommendations are based on ac-tual user ratings. In addition, artist variety in recommended pieces tends to be poor. The latter, which recommends mu-sical pieces that are similar to users' favorites in terms of music content, has not been fully investigated. This induces unreliability in modeling of user preferences; the content similarity does not completely reflect the preferences. Our method integrates both rating and content data by using a Bayesian network called an aspect model. Unobservable user preferences are directly represented by introducing la-tent variables, which are statistically estimated. To verify our method, we conducted experiments by using actual au-dio signals of Japanese songs and the corresponding rating data collected from Amazon. The results showed that our method outperforms the two conventional methods in terms of recommendation accuracy and artist variety and can rea-sonably recommend pieces even if they have no ratings.},
    author = {Yoshii, Kazuyoshi and Goto, Masataka and Komatani, Kazunori and Ogata, Tetsuya and Okuno, Hiroshi G},
    file = {:C$\backslash$:/Users/Philipp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yoshii et al. - Unknown - Hybrid Collaborative and Content-based Music Recommendation Using Probabilistic Model with Latent User Prefere.pdf:pdf},
    journal = {{Proceedings of the 7th International Conference on Music Information Retrieval (ISMIR\}},
    pages = {296----301},
    title = {{Hybrid Collaborative and Content-based Music Recommendation Using Probabilistic Model with Latent User Preferences}},
    year = {2006}
}
